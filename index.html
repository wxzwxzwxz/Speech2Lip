<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Speech2Lip: High-fidelity Speech to Lip Generation by Learning from
              a Short
              Video</h1>
            <div class="centered">
              <div class="is-size-6 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  Xiuzhe Wu<sup>1</sup>,</span>
                <span class="author-block">
                  Pengfei Hu</a><sup>2</sup>,</span>
                <span class="author-block">
                  Yang Wu</a><sup>3,4</sup>,</span>
                <span class="author-block">
                  Xiaoyang Lyu</a><sup>1</sup>,</span>
                <span class="author-block">
                  Yan-Pei Cao</a><sup>3</sup>,</span>
                <span class="author-block">
                  Ying Shan</a><sup>3</sup>,</span>
                <span class="author-block">
                  Wenming Yang</a><sup>3</sup>,</span>
                <span class="author-block">
                  Zhongqian Sun</a><sup>4</sup>,</span>
                <span class="author-block">
                  Xiaojuan Qi</a><sup>1</sup></span>
              </div>
            </div>

            <div class="is-size-6 publication-authors centered">
              <span class="author-block"><sup>1</sup>The University of Hong Kong,<sup>2</sup>Tsinghua
                University,<sup>3</sup> ARC Lab, Tencent PCG,<sup>4</sup> Tencent AI Lab<br>ICCV 2023</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2309.04814" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->


                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/CVMI-Lab/Speech2Lip" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code(coming soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2309.04814" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="static/videos/banner_video.mp4" type="video/mp4">
        </video>
        <!-- <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2> -->
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Synthesizing realistic videos according to a given speech
              is still an open challenge. Previous works have been
              plagued by issues such as inaccurate lip shape generation
              and poor image quality. The key reason is that only motions
              and appearances on limited facial areas (e.g., lip area) are
              mainly driven by the input speech. Therefore, directly learning a mapping function from speech to the
              entire head image
              is prone to ambiguity, particularly when using a short video
              for training. We thus propose a decomposition-synthesis composition framework named Speech to Lip
              (Speech2Lip)
              that disentangles speech-sensitive and speech-insensitive
              motion/appearance to facilitate effective learning from limited training data, resulting in the generation
              of naturallooking videos. First, given a fixed head pose (i.e., canonical space), we present a
              speech-driven implicit model for lip
              image generation which concentrates on learning speechsensitive motion and appearance. Next, to model the
              major
              speech-insensitive motion (i.e., head movement), we introduce a geometry-aware mutual explicit mapping
              (GAMEM)
              module that establishes geometric mappings between different head poses. This allows us to paste generated
              lip
              images at the canonical space onto head images with arbitrary poses and synthesize talking videos with
              natural head
              movements. In addition, a Blend-Net and a contrastive
              sync loss are introduced to enhance the overall synthesis
              performance. Quantitative and qualitative results on three
              benchmarks demonstrate that our model can be trained by
              a video of just a few minutes in length and achieve stateof-the-art performance in both visual quality and
              speechvisual synchronization.
            </p>
            <img src="./static/images/picture1.png" alt="Description of the image">
            <p>Given a speech as input, our model generates high quality talking-head videos and supports pose-controllable synthesis. The decomposition and synthesis modules make learning from
              a short video more effective and the composition module enables
              us to synthesize high-fidelity videos.
              </p>
          </div>
        </div>
      </div>
    </div>



    <section class="section is-small" style="background-color:white;">
      <!-- 原始内容 -->
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full">
            <div class="content">
              <h2 class="title is-3">Motivation</h2>
              <div class="level-set has-text-justified">
                <p>
                  Talking videos contain various motions (e.g. lip, head, torso).
                  <br>
                  Only limited areas (e.g. lip) are highly associated with speech.
                  <br>
                  Simplifying modeling of motions and appearances in speech-sensitive area is critical for effective learning from a short video.
                  
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>



    <section class="section is-small" style="background-color:palegray;">
      <!-- 原始内容 -->
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full">
            <div class="content">
              <h2 class="title is-3">Preliminary Experiment</h2>
              <div class="level-set has-text-justified">
                <p>
                  We apply warping to all captured images with varying head poses (observed views), bring them to a fixed head pose (canonical view) and compute heatmaps in both views.
                </p>
                <div class="center-image"> <!-- 添加一个新的包裹元素 -->
                  <img src="./static/images/pictuer2.png" >
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    
    <style>
      .center-image { /* 新的 CSS 样式 */
        display: flex;
        justify-content: center;
      }
    </style>




<section class="section is-small" style="background-color:white;">
  <!-- 原始内容 -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Observation</h2>
          <div class="level-set has-text-justified">
            <p>
              Left: without eliminating other motions, many areas are affected by the speech.
<br>
Right: with eliminating other motions, limited areas display high sensitivity to the speech.
            </p>
            <div class="center-image"> <!-- 添加一个新的包裹元素 -->
              <img src="./static/images/picture4.png" >
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section is-small" style="background-color:palegray;">
  <!-- 原始内容 -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Overall Framework
          </h2>
          <div class="level-set has-text-justified">
            <p>
              We model them individually (a), and compose the ultimate output image (b). The synchronization performance enhancement and GAMEM are illustrated in (c) and (d).

            </p>
            <div class="center-image"> <!-- 添加一个新的包裹元素 -->
               <img src="./static/images/picture5.png" >
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section is-small" style="background-color:white;">
  <!-- 原始内容 -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Main Results

          </h2>
          <div class="level-set has-text-justified">
            <p>
              Quantitative Results


            </p>
            <div class="center-image"> <!-- 添加一个新的包裹元素 -->
               <img src="./static/images/picture6.png" >
            </div>
            <p>Ablation Study
            </p>
            <div class="center-image"> <!-- 添加一个新的包裹元素 -->
              <img src="./static/images/picture7.png">
           </div>
           <p>Qualitative Results
          </p>
          <div class="center-image"> <!-- 添加一个新的包裹元素 -->
            
            <img src="./static/images/Qualitative Results.png" alt="">
         </div>
         <p><strong>Acknowledgement:</strong> This work has been supported by the Research Fund from Tencent ARC lab.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




  <!-- End paper abstract -->


  <!-- Image carousel -->
  <!-- <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item"> -->
            <!-- Your image here -->
            <!-- <img src="static/images/carousel1.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              First image description.
            </h2>
          </div>
          <div class="item"> -->
            <!-- Your image here -->
            <!-- <img src="static/images/carousel2.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Second image description.
            </h2>
          </div>
          <div class="item"> -->
            <!-- Your image here -->
            <!-- <img src="static/images/carousel3.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Third image description.
            </h2>
          </div>
          <div class="item"> -->
            <!-- Your image here -->
            <!-- <img src="static/images/carousel4.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Fourth image description.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End image carousel -->




  <!-- Youtube video -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <h2 class="title is-3">Video Presentation</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">

            <div class="publication-video">
              <!-- Youtube embed code here -->
              <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media"
                allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End youtube video -->


  <!-- Video carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Another Carousel</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="static/videos/carousel1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video2">
            <video poster="" id="video2" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="static/videos/carousel2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video3">
            <video poster="" id="video3" autoplay controls muted loop height="100%">\
              <!-- Your video file here -->
              <source src="static/videos/carousel3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End video carousel -->






  <!-- Paper poster -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section>
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
